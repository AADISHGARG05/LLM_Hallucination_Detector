# ğŸ§  Multi-Signal Hallucination Detection System for LLM-Generated Text

An **explainable, end-to-end machine learning system** that detects **hallucination risk** in AI-generated text using **multiple complementary signals**, including a fine-tuned BERT classifier, semantic consistency analysis, and linguistic uncertainty detection.

> âš ï¸ This project **does NOT perform fact-checking or web search**.  
> Instead, it detects **hallucination patterns** learned from data.

---

## ğŸ”— Live Demo (Deployment)

ğŸš€ **Deployment Link:**  
ğŸ‘‰ *Will be updated soon*

---

## ğŸ“Œ Problem Statement

Large Language Models (LLMs) such as ChatGPT, Gemini, and Claude often generate text that is:

- Fluent and confident
- Grammatically correct
- **Factually incorrect or fabricated (hallucinated)**

This is risky in domains like:
- Education
- Healthcare
- Research
- Enterprise AI systems

Most existing solutions rely on **search, RAG, or fact verification**, which are:
- Heavy to deploy
- Costly
- Infrastructure-dependent

---

## ğŸ¯ Project Goal

To build a **lightweight, ML-driven hallucination detection layer** that:

- Works **without external search**
- Uses **learned and linguistic patterns**
- Produces an **explainable hallucination risk score**
- Can be easily integrated into real-world systems

---

## ğŸ§  Core Idea

> **Hallucination is not a single signal â€” it is a pattern.**

This system detects hallucinations by combining **multiple independent signals**, rather than relying on a single model.

---

## ğŸ—ï¸ System Architecture

LLM-Generated Text
â†“
Signal 1: BERT-Based Hallucination Classifier
Signal 2: Semantic Consistency Analysis
Signal 3: Linguistic Uncertainty Analysis
â†“
Weighted Risk Aggregation
â†“
Hallucination Risk Score (Low / Medium / High)
â†“
Web UI (Flask + HTML/CSS/JS)

---

## ğŸ” Hallucination Signals

### ğŸ”¹ Signal 1: ML / DL Classifier (Core Signal)

- **Model:** Fine-tuned `bert-base-uncased`
- **Datasets:**
  - HaluEval
  - TruthfulQA (converted to binary labels)
- **Output:** Probability that text is hallucinated

ğŸ“Œ Demonstrates **supervised ML + deep learning skills**

---

### ğŸ”¹ Signal 2: Semantic Consistency Analysis

**Intuition:**  
Hallucinated text often:
- Jumps between topics
- Lacks internal coherence
- Contains weak semantic flow

**Method:**
- Split text into sentences
- Generate sentence embeddings (Sentence-BERT)
- Compute average cosine similarity

Low consistency â†’ higher hallucination risk

---

### ğŸ”¹ Signal 3: Linguistic Uncertainty Analysis

**Intuition:**  
Hallucinated answers often contain:
- Hedging language (`may`, `might`, `possibly`)
- Vague phrasing
- Irregular sentence structure

**Features Used:**
- Hedging word density
- Modal verb frequency
- Sentence length variance

---

## ğŸ“Š Risk Aggregation (Explainable)

Final hallucination risk is computed as a **weighted combination**:

```text
Final Risk =
0.45 Ã— Model Probability
+ 0.40 Ã— (1 âˆ’ Semantic Consistency)
+ 0.15 Ã— Linguistic Uncertainty
```

ğŸ“‚ Project Structure
llm-hallucination-detector/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”œâ”€â”€ signals/
â”‚   â”‚   â”œâ”€â”€ semantic.py
â”‚   â”‚   â””â”€â”€ linguistic.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ analyze.html
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ 01_data_preparation.ipynb
â”‚   â”œâ”€â”€ 02_ml_models.ipynb
â”‚   â””â”€â”€ 03_bert_classifier.ipynb
â”‚
â”œâ”€â”€ trained_models/
â”‚   â”œâ”€â”€ bert_hallucination/
â”‚   â”œâ”€â”€ logreg.pkl
â”‚   â”œâ”€â”€ rf.pkl
â”‚   â””â”€â”€ tfidf.pkl
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



ğŸ‘¨â€ğŸ’» Author

Aadish Garg
AI / Machine Learning Enthusiast
ğŸ”— LinkedIn: https://www.linkedin.com/in/aadish-garg/
ğŸ’» GitHub: https://github.com/AADISHGARG05
